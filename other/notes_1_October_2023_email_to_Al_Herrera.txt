Hi Al,

Last week, when we tried to use LLM aided queries to improve retrieval, we ran into token limit issues.  While exploring token limit issues I made a serendipitous discovery: in certain cases, token limit is more important than having a smarter model!
There are two attachments illustrating a difference in performance with gpt-4 (smater but with 8-k token limit) and gtp-3.5-turbo-16k, not quite as smart but with a16 k token limit.  Notice that for the complex questions, gpt-3.5-turbo-16k solidly defeated gpt-4!  And, moreover, this test is repeatable.  I repeated the test three times for each model and got really consistent results, which implies that while gpt-3.5-turbo-16k isn't quite as smart, in terms of reasoning horsepower, it can outperform with a broader context.
This is very important, and I'm writing it here for documentation purposes and will add notes to the Github repo to document this as well.  Complex questions often require a broad base of input.  And an 8-k token limit constraints the complexity of the problem that you can solve in certain cases because the solution can require a very broad context that may not fit into a small token limit!
What are the possible solutions?  Well, there is 'gtp-4-32k' but that model is NOT as of this writing on 1 October 2023 available for API usage due to the fact OpenAI is struggling heavily with compute resources!  
Now when I reduced the model to gpt-3.5-turbo, I got an error message: "openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4379 tokens. Please reduce the length of the messages."  This is similar to previous error messages we saw last week, so it looks like this problem is already creating a major strain against token limits.
I'll document this and we'll discuss.  I see some future directions here:
1. I still need to explore alternate retrieval methods that use knowledge graphs.  This isn't the work of a single week (I hope I'm wrong but am afraid I'm not).And we should consider have a summarization system that can allow searching summaries and doing search hierarchy based on contents and on summaries of documents.  So search a top level summary to know which book and chapter, search a bit more to learn which pages, search a bit more to get the relevant passages and context.  So multi-level summary hierarchy could also be useful in intelligent machines that must solve high complexity issues.
Ryan
