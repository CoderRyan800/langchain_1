{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d28baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanmukai/anaconda3/envs/lang_chain_py38/lib/python3.8/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/home/ryanmukai/Documents/github/redesigned-octo-goggles/writing/lstm_paper.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ae255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b019db-0d85-4383-8eb9-687aef044349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffec3284-c4bd-4916-bd38-58a3dca92746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc437fb8-a302-4d4f-8bd5-074e4624b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Simple Reasoning and Knowledge States in a\\nLSTM-Based Agent\\nRyan Mukai\\nJune 27, 2020\\n1 Introduction\\nThis article focuses on the development of a simple form of self-awareness in an\\nLSTM-based agent. We present an agent capable of displaying its knowledge\\nstate and of answering questions based on its state of knowledge. If an agent\\nis unable to answer a question, it will indicate this and request assistance from\\nanother agent. The other agent, upon receiving such a request, provides data\\nfrom its knowledge state to aid the requester in its goal of \\x0cnding an answer.\\nThe goals of this work are:\\n1. Having agents maintain a concept of a propositional sentence as a unit of\\nthought.\\n2. Having agents possess a concept of their own knowledge in the sense of\\nbeing able to dump their knowledge state on request.\\n3. Having agents possess a concept of their own knowledge in the sense of\\nbeing aware of not being able to answer a question and asking for help.\\n4. Having agents take advantage of basic self-knowledge to cooperate in order\\nto solve a simple problem.\\n5. Having agents be aware of contradictions in their knowledge. In proposi-\\ntional logic, a contradiction (i.e. False) can imply anything, and the agent\\nshould warn of a contradiction in its knowledge if it recognizes one.\\nHence, our de\\x0cnition of self-awareness is a very narrow one that focuses on\\ngiving agents the ability to know certain things about their own knowledge state,\\nand it is clear that this is far narrower than anything approaching conscious-\\nness. The focus of this article is not on creating a general-purpose reasoning\\nagent since existing literature already covers this in much depth. The focus is\\ninstead on having neural network-based agents base their actions on their own\\nunderstanding of their state of knowledge and use that knowledge to cooperate\\nto solve a problem.\\n1' metadata={'source': '/home/ryanmukai/Documents/github/redesigned-octo-goggles/writing/lstm_paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488c231b-a2f8-4b9d-8b8e-f5a81b910b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store \n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "vectorstore = Chroma.from_documents(documents=pages,embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398d336d-f9fc-47eb-abf7-83ddaf9f37ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How is self-awareness defined?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77be0ee6-011e-423b-9878-3719cea6c3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Simple Reasoning and Knowledge States in a\\nLSTM-Based Agent\\nRyan Mukai\\nJune 27, 2020\\n1 Introduction\\nThis article focuses on the development of a simple form of self-awareness in an\\nLSTM-based agent. We present an agent capable of displaying its knowledge\\nstate and of answering questions based on its state of knowledge. If an agent\\nis unable to answer a question, it will indicate this and request assistance from\\nanother agent. The other agent, upon receiving such a request, provides data\\nfrom its knowledge state to aid the requester in its goal of \\x0cnding an answer.\\nThe goals of this work are:\\n1. Having agents maintain a concept of a propositional sentence as a unit of\\nthought.\\n2. Having agents possess a concept of their own knowledge in the sense of\\nbeing able to dump their knowledge state on request.\\n3. Having agents possess a concept of their own knowledge in the sense of\\nbeing aware of not being able to answer a question and asking for help.\\n4. Having agents take advantage of basic self-knowledge to cooperate in order\\nto solve a simple problem.\\n5. Having agents be aware of contradictions in their knowledge. In proposi-\\ntional logic, a contradiction (i.e. False) can imply anything, and the agent\\nshould warn of a contradiction in its knowledge if it recognizes one.\\nHence, our de\\x0cnition of self-awareness is a very narrow one that focuses on\\ngiving agents the ability to know certain things about their own knowledge state,\\nand it is clear that this is far narrower than anything approaching conscious-\\nness. The focus of this article is not on creating a general-purpose reasoning\\nagent since existing literature already covers this in much depth. The focus is\\ninstead on having neural network-based agents base their actions on their own\\nunderstanding of their state of knowledge and use that knowledge to cooperate\\nto solve a problem.\\n1', metadata={'page': 0, 'source': '/home/ryanmukai/Documents/github/redesigned-octo-goggles/writing/lstm_paper.pdf'}),\n",
       " Document(page_content='3.1 Agent Cooperation: Example 1\\nSuppose we have two agents, Agent 1 and Agent 2. Agent 1 is given the following\\nknowledge:\\n\\x0fA == >B\\nAgent 2 is given the following knowledge:\\n\\x0fA\\nIf we ask Agent 1, \\\\What is B ?\", the following will occur.\\n1. Agent 1 indicates B is unknown and requests help.\\n2. The simulation environment, in response to seeing Agent 1 request help,\\nsends a help query to Agent 2.\\n3. Agent 2 dumps its knowledge that A is true.\\n4. This knowledge is added to the knowledge base of Agent 1.\\n5. Agent 1 runs again and is able to \\x0cnally conclude B is true.\\nIn this example, we note the following:\\n1. Agent 1 lacks adequate information to determine B.\\n2. Agent 1, aware of its lack of knowledge, makes a request for help.\\n3. Agent 2 responds to a help query by dumping its knowledge state.\\n4. This allows Agent 1 to \\x0cnd the answer.\\nFrom this example:\\n1. Agents know whether or not they know an answer and requesting help\\nwhen they do not.\\n2. Agents also have knowledge states that they can dump if they are asked\\nfor help.\\n3. In terms of maintaining an internal knowledge state, an agent knows how\\nto purge repeat sentences and understands, to some extent, the idea of\\na sentence as a unit of knowledge since it can repeat units of knowledge\\nwhen asked.\\n4', metadata={'page': 3, 'source': '/home/ryanmukai/Documents/github/redesigned-octo-goggles/writing/lstm_paper.pdf'}),\n",
       " Document(page_content='Much work has occurred the \\x0celd of neural networks and symbolic reason-\\ning. An old survey of the \\x0celd, with numerous references to other developments,\\nis [4], which is also a nice introduction to the \\x0celd. An older introduction to\\nneural-symbolic learning is [8], and another overview of the \\x0celd is in [5]. Ref-\\nerence [11] describes many approaches used in neuro-symbolic learning systems,\\nincluding the SHRUTI spiking neural network [18], the Core Method for learn-\\ning \\x0crst order logic programs [3], topos theory for learning models of predicate\\nlogical theories [7], modal and temporal reasoning with neural networks [14], and\\nmulti-valued logic and neural networks [12], and many other approaches have\\nbeen described in the literature. The use of LSTMs for neuro-symbolic tasks\\nalso has a precedent, and one example is [13]. In [10], the authors introduce\\nPossibleWorldNets capable of achieving very good performance on propositional\\nlogic problems. A very interesting example of symbolic mathematical process-\\ning is [15] in which a neural network was able to beat Mathematica on complex\\nsymbolic mathematical tasks.\\nSelf-awareness in neural networks has also been studied, and a survey paper\\nis [9]. Our work, which emphasizes a type of self-awareness, di\\x0bers from the\\nmethods presented in this survey because the aforementioned methods focus on\\nintrospection at the architectural, physical, and circuit layers. By contrast, this\\nworks focuses on a simple form of symbolic, conceptual self-awareness not at\\nthe neural layer but at the level of logic concepts.\\n2 The Problem Agents are Trained to Solve\\nAgents are trained to reason on sentences involving syllogisms and the logical\\nnot(~), and (&), or ( j), exclusive-or (^), implication (== >), and bidirectional\\nimplication ( <==>) operators. Throughout this article propositions, which can\\nbe either True or False, are denoted by the capital letters A through J inclusive.\\nWe train agents to handle simple problems of the following forms.\\n1. Given the status of a propositional variable A as true, false, or unknown,\\nrespond to a question about its value.\\n2. Given a simple sentence such as A == >B and given A is true, answer\\nthat B is true if asked about B.\\n3. Given a simple sentence such as A == >B and given B is false, answer\\nthat A is false if asked about A.\\n4. Given the value of A and no other information, if asked about B indicate\\nB is unknown.\\n5. Given contradictory statements, warn that the knowledge base is contra-\\ndictory.\\n6. In addition to sentences of the form A == >B, we include sentences of\\nthe form (AjB), (A ^ B), (A <==>B), and (A & B).\\n2', metadata={'page': 1, 'source': '/home/ryanmukai/Documents/github/redesigned-octo-goggles/writing/lstm_paper.pdf'}),\n",
       " Document(page_content='3.2 A Second Example\\nAgent 1 is given the following knowledge:\\n\\x0fA == >B\\nAgent 2 is given the following knowledge:\\n\\x0f~B\\nIf we ask Agent 1, \\\\What is A?\", the following will occur.\\n1. Agent 1 indicates a is unknown and requests help.\\n2. The simulation environment, in response to seeing Agent 1 request help,\\nsends a help query to Agent 2.\\n3. Agent 2 dumps its knowledge ~B.\\n4. This knowledge is added to the knowledge base of Agent 1.\\n5. Agent 1 runs again and is able to \\x0cnally conclude A is false.\\n4 Key Results\\nThe network was found to yield an error rate of slightly less than 1% on\\nthe validation data set logic_data_extended.tsv contained in the gzip tar\\narchive at https://beta1-demo.s3.amazonaws.com/beta_demo_data_files.\\ntgz, which we have made publicly available to facilitate peer review. In this\\ntab-separated \\x0cle, the columns are de\\x0cned as follows:\\n1. The \\x0crst column, or column 0, contains a set of logical statements sep-\\narated by the logical and (&) operator. This is the input data to the\\nnetwork consisting of logical statements and a question about a variable.\\n2. The second column, or column 1, contains the same set condensed, with-\\nout any repetitions. It also contains the same question about a variable\\nas column 0. This column was used while experimenting with training\\nwithout repeated sentences during earlier stages of this research, but it\\nwas ignored in the \\x0cnal work and column 0 with possible repetition was\\nused instead.\\n3. The third column, or column 2, is much like the second column, but\\nwithout the question. This column contains the correct answer to the\\n\\\\HELP\" query.\\n4. The fourth column, or column 3, contains the answer to the question in\\ncolumn 0, which could be: True, False, Contradictory, or Unknown HELP!\\nThe network is trained to respond to the question with the answer from\\ncolumn 3 (fourth column). It is also trained to respond to the sentences, but\\nwith the question replaced by HELP, by yielding a condensed form of knowledge\\nfrom column 2 (third column).\\n5', metadata={'page': 4, 'source': '/home/ryanmukai/Documents/github/redesigned-octo-goggles/writing/lstm_paper.pdf'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb93a327-be2d-41ce-b9e5-7dc6d521b9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How is self-awareness defined?',\n",
       " 'result': \"In the context of the article, self-awareness is defined as the ability of agents (LSTM-based agents in this case) to know certain things about their own knowledge state. It refers to the agents' understanding of their own knowledge, their ability to recognize when they don't know the answer to a question, and their capability to request help from other agents. However, it is important to note that this definition of self-awareness is narrow and does not encompass consciousness or general-purpose reasoning.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f34de23e-7eea-491a-92da-161a3bc8f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To improve this paper for publication in a peer-reviewed journal, consider the following suggestions:\n",
      "\n",
      "1. Clarify the research problem: Clearly state the research problem and its significance in the introduction. Explain why the problem is important and how your work contributes to the existing literature.\n",
      "\n",
      "2. Provide a comprehensive literature review: Expand the literature review section to include a thorough review of relevant studies and approaches in the field of neural networks and symbolic reasoning. Discuss the limitations of previous work and highlight the novelty of your approach.\n",
      "\n",
      "3. Methodology and experimental setup: Provide a detailed description of the methodology and experimental setup used in your study. Explain the rationale behind your choice of neural network architecture and training techniques. Include information about the dataset used, its source, and any preprocessing steps applied.\n",
      "\n",
      "4. Results and analysis: Present the results of your experiments in a clear and concise manner. Include relevant metrics and statistical analysis to support your findings. Discuss the implications of your results and how they contribute to solving the research problem.\n",
      "\n",
      "5. Discussion and interpretation: Provide a comprehensive discussion of your results, comparing them with previous studies and explaining any discrepancies or similarities. Interpret the findings in the context of the research problem and discuss their implications for future research.\n",
      "\n",
      "6. Address limitations and future directions: Acknowledge any limitations or constraints of your study and suggest potential areas for future research. Discuss how the limitations could be overcome and propose additional experiments or extensions to further validate your approach.\n",
      "\n",
      "7. Writing style and clarity: Ensure that the paper is well-organized, coherent, and written in clear and concise language. Use appropriate headings and subheadings to structure the content. Proofread the paper for grammar, spelling, and punctuation errors.\n",
      "\n",
      "8. Citations and references: Ensure that all sources are properly cited and referenced according to the journal's guidelines. Use a consistent citation style throughout the paper.\n",
      "\n",
      "9. Peer review and feedback: Seek feedback from colleagues or experts in the field to get their input on the paper. Incorporate their suggestions and address any concerns or criticisms raised.\n",
      "\n",
      "10. Follow journal guidelines: Familiarize yourself with the specific guidelines and requirements of the target journal. Ensure that your paper adheres to the journal's formatting, length, and submission guidelines.\n",
      "\n",
      "By implementing these suggestions, you can enhance the clarity, rigor, and overall quality of your paper, increasing its chances of being accepted for publication in a peer-reviewed journal.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you please provide advice on how to improve this paper so it can be published in a peer reviewed journal?\"\n",
    "result=qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2350d-cefa-47fa-b526-e1750eb149cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
